{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def read_file(file_path):\n",
    "    with open(file_path,'r',encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "        \n",
    "    row = lines[0]\n",
    "    feature_name = row.strip().split('\\t')[1:]\n",
    "    \n",
    "    row_name = []\n",
    "    data = []\n",
    "    for line in lines[1:]:\n",
    "        row_data = line.strip().split('\\t')\n",
    "        row_name.append(row_data[0])\n",
    "        data.append([float(i) for i in row_data[1:]])\n",
    "    return feature_name,row_name,data\n",
    "        \n",
    "key_words,blog_titles,data = read_file('blogdata.txt')\n",
    "\n",
    "\n",
    "# key_words.index('yahoo') # 3\n",
    "# blog_titles.index('Quick Online Tips') # 85\n",
    "# data[85][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建的新的聚类结构\n",
    "class bicluster:\n",
    "    def __init__(self,vec,left=None,right=None,distance=0.0,id=None):\n",
    "        self.left = left # 指向左子类\n",
    "        self.right = right # 指向右子类\n",
    "        self.vec = vec # 上文的data,即 词的频率\n",
    "        self.id = id # 文档的ID\n",
    "        self.distance = distance # 左子类和右子类的相似度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "def pearson(v1,v2):\n",
    "    sum1 = sum(v1)\n",
    "    sum2 = sum(v2)\n",
    "    \n",
    "    sum1sq = sum([pow(v,2) for v in v1])\n",
    "    sum2sq = sum([pow(v,2) for v in v2])\n",
    "    \n",
    "    multi_sum = sum([v1[i] * v2[i] for i in range(len(v1))])\n",
    "    \n",
    "    num = multi_sum - (sum1 * sum2 / len(v1))\n",
    "    den = sqrt((sum1sq - pow(sum1, 2) / len(v1)) * (sum2sq - pow(sum2,2) / len(v2)))\n",
    "    if den == 0:\n",
    "        return 0\n",
    "    return 1.0 - num/den"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "聚类函数 核心\n",
    "算法核心是将文档两两pearson值计算，将两个距离最小的合并成一个新类，同时利用树的结构保存原来的关系，直至只有一个大类，算法结束\n",
    "'''\n",
    "def hcluster(rows,distance=pearson):\n",
    "    # 初始时每个数据都是一类，用列表存储\n",
    "    clust = [bicluster(rows[i],id=i) for i in range(len(rows))]\n",
    "    \n",
    "    distance_pair = {}\n",
    "    currentclustid=-1 # -1 表示构建生成的类ID\n",
    "    # 得到树根（即一个类）时退出循环\n",
    "    while len(clust) > 1:\n",
    "        lowestpair = (0,1)\n",
    "        closest = distance(clust[0].vec, clust[1].vec)\n",
    "        # 两两对比，寻找最小距离\n",
    "        for i in range(len(clust)):\n",
    "            for j in range(i+1, len(clust)):\n",
    "                if (clust[i].id, clust[j].id) not in distance_pair:\n",
    "                    distance_pair[(clust[i].id, clust[j].id)] = distance(clust[i].vec, clust[j].vec)\n",
    "                \n",
    "                d = distance_pair[(clust[i].id, clust[j].id)]\n",
    "                if d < closest:\n",
    "                    closest = d\n",
    "                    lowestpair = (i,j)\n",
    "        \n",
    "        # 计算两个最小距离对应的类的平均值\n",
    "        mergevec=[(clust[lowestpair[0]].vec[i]+clust[lowestpair[1]].vec[i])/2.0 for i in range(len(clust[0].vec))]\n",
    "        # 将上面两个类合并生成新的类，左右分支分别指向原来的类\n",
    "        newcluster=bicluster(mergevec,left=clust[lowestpair[0]],right=clust[lowestpair[1]],distance=closest,id=currentclustid)\n",
    "        \n",
    "        currentclustid-=1\n",
    "        del clust[lowestpair[1]]\n",
    "        del clust[lowestpair[0]]\n",
    "        clust.append(newcluster)\n",
    "\n",
    "    return clust[0],distance_pair\n",
    "\n",
    "result,result1 = hcluster(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      " gapingvoid: \"cartoons drawn on the back of business cards\"\n",
      " -\n",
      "  -\n",
      "   Schneier on Security\n",
      "   Instapundit.com\n",
      "  -\n",
      "   The Blotter\n",
      "   -\n",
      "    -\n",
      "     MetaFilter\n",
      "     -\n",
      "      SpikedHumor\n",
      "      -\n",
      "       Captain's Quarters\n",
      "       -\n",
      "        Michelle Malkin\n",
      "        -\n",
      "         -\n",
      "          NewsBusters.org - Exposing Liberal Media Bias\n",
      "          -\n",
      "           -\n",
      "            Hot Air\n",
      "            Crooks and Liars\n",
      "           -\n",
      "            Power Line\n",
      "            Think Progress\n",
      "         -\n",
      "          Andrew Sullivan | The Daily Dish\n",
      "          -\n",
      "           Little Green Footballs\n",
      "           -\n",
      "            Eschaton\n",
      "            -\n",
      "             Talking Points Memo: by Joshua Micah Marshall\n",
      "             Daily Kos\n",
      "    -\n",
      "     43 Folders\n",
      "     -\n",
      "      TechEBlog\n",
      "      -\n",
      "       -\n",
      "        Mashable!\n",
      "        Signum sine tinnitu--by Guy Kawasaki\n",
      "       -\n",
      "        -\n",
      "         -\n",
      "          Slashdot\n",
      "          -\n",
      "           MAKE Magazine\n",
      "           Boing Boing\n",
      "         -\n",
      "          -\n",
      "           Oilman\n",
      "           -\n",
      "            Online Marketing Report\n",
      "            -\n",
      "             Treehugger\n",
      "             -\n",
      "              SimpleBits\n",
      "              -\n",
      "               Cool Hunting\n",
      "               -\n",
      "                Steve Pavlina's Personal Development Blog\n",
      "                -\n",
      "                 -\n",
      "                  ScienceBlogs : Combined Feed\n",
      "                  Pharyngula\n",
      "                 -\n",
      "                  BuzzMachine\n",
      "                  -\n",
      "                   Copyblogger\n",
      "                   -\n",
      "                    -\n",
      "                     The Viral Garden\n",
      "                     Seth's Blog\n",
      "                    -\n",
      "                     -\n",
      "                      Bloggers Blog: Blogging the Blogsphere\n",
      "                      -\n",
      "                       Sifry's Alerts\n",
      "                       ProBlogger Blog Tips\n",
      "                     -\n",
      "                      -\n",
      "                       Valleywag\n",
      "                       Scobleizer - Tech Geek Blogger\n",
      "                      -\n",
      "                       -\n",
      "                        O'Reilly Radar\n",
      "                        456 Berea Street\n",
      "                       -\n",
      "                        Lifehacker\n",
      "                        -\n",
      "                         Quick Online Tips\n",
      "                         -\n",
      "                          Publishing 2.0\n",
      "                          -\n",
      "                           Micro Persuasion\n",
      "                           -\n",
      "                            A Consuming Experience (full feed)\n",
      "                            -\n",
      "                             John Battelle's Searchblog\n",
      "                             -\n",
      "                              Search Engine Watch Blog\n",
      "                              -\n",
      "                               Read/WriteWeb\n",
      "                               -\n",
      "                                Official Google Blog\n",
      "                                -\n",
      "                                 Search Engine Roundtable\n",
      "                                 -\n",
      "                                  Google Operating System\n",
      "                                  Google Blogoscoped\n",
      "          -\n",
      "           -\n",
      "            -\n",
      "             -\n",
      "              Blog Maverick\n",
      "              -\n",
      "               Download Squad\n",
      "               -\n",
      "                CoolerHeads Prevail\n",
      "                -\n",
      "                 Joystiq\n",
      "                 The Unofficial Apple Weblog (TUAW)\n",
      "             -\n",
      "              Autoblog\n",
      "              -\n",
      "               Engadget\n",
      "               TMZ.com\n",
      "            -\n",
      "             Matt Cutts: Gadgets, Google, and SEO\n",
      "             -\n",
      "              PaulStamatiou.com\n",
      "              -\n",
      "               -\n",
      "                GigaOM\n",
      "                TechCrunch\n",
      "               -\n",
      "                -\n",
      "                 Techdirt\n",
      "                 Creating Passionate Users\n",
      "                -\n",
      "                 Joho the Blog\n",
      "                 -\n",
      "                  -\n",
      "                   PerezHilton.com\n",
      "                   Jeremy Zawodny's blog\n",
      "                  -\n",
      "                   Joi Ito's Web\n",
      "                   -\n",
      "                    ongoing\n",
      "                    -\n",
      "                     Joel on Software\n",
      "                     -\n",
      "                      -\n",
      "                       we make money not art\n",
      "                       -\n",
      "                        plasticbag.org\n",
      "                        -\n",
      "                         Signal vs. Noise\n",
      "                         -\n",
      "                          kottke.org\n",
      "                          -\n",
      "                           Neil Gaiman's Journal\n",
      "                           -\n",
      "                            -\n",
      "                             The Huffington Post | Raw Feed\n",
      "                             -\n",
      "                              Wonkette\n",
      "                              -\n",
      "                               Gawker\n",
      "                               -\n",
      "                                The Superficial - Because You're Ugly\n",
      "                                Go Fug Yourself\n",
      "                            -\n",
      "                             Deadspin\n",
      "                             Gothamist\n",
      "                      -\n",
      "                       Kotaku\n",
      "                       Gizmodo\n",
      "           -\n",
      "            Shoemoney - Skills to pay the bills\n",
      "            -\n",
      "             flagrantdisregard\n",
      "             -\n",
      "              WWdN: In Exile\n",
      "              -\n",
      "               Derek Powazek\n",
      "               -\n",
      "                lifehack.org\n",
      "                Dave Shea's mezzoblue\n",
      "        -\n",
      "         Wired News: Top Stories\n",
      "         -\n",
      "          Topix.net Weblog\n",
      "          Bloglines | News\n"
     ]
    }
   ],
   "source": [
    "# 字符版打印聚类，理解成树\n",
    "def printclust(clust,labels=None,n=0):\n",
    "    for i in range(n): \n",
    "        print(' ',end=\"\")\n",
    "    if clust.id<0:\n",
    "        print('-')\n",
    "    else:\n",
    "        if labels==None:\n",
    "            print(clust.id)\n",
    "        else:\n",
    "            print(labels[clust.id])\n",
    "\n",
    "    if clust.left!=None:\n",
    "        printclust(clust.left,labels=labels,n=n+1)\n",
    "    if clust.right!=None:\n",
    "        printclust(clust.right,labels=labels,n=n+1)\n",
    "        \n",
    "printclust(result,blog_titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image,ImageDraw\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2,\n",
       "  5,\n",
       "  7,\n",
       "  13,\n",
       "  16,\n",
       "  21,\n",
       "  23,\n",
       "  24,\n",
       "  25,\n",
       "  30,\n",
       "  34,\n",
       "  36,\n",
       "  40,\n",
       "  48,\n",
       "  49,\n",
       "  56,\n",
       "  63,\n",
       "  65,\n",
       "  67,\n",
       "  68,\n",
       "  69,\n",
       "  79,\n",
       "  80,\n",
       "  84,\n",
       "  85,\n",
       "  91,\n",
       "  94],\n",
       " [4, 41, 42, 45, 46, 81],\n",
       " [0,\n",
       "  1,\n",
       "  3,\n",
       "  6,\n",
       "  8,\n",
       "  9,\n",
       "  10,\n",
       "  11,\n",
       "  12,\n",
       "  14,\n",
       "  15,\n",
       "  17,\n",
       "  18,\n",
       "  19,\n",
       "  20,\n",
       "  22,\n",
       "  26,\n",
       "  27,\n",
       "  28,\n",
       "  29,\n",
       "  31,\n",
       "  32,\n",
       "  33,\n",
       "  35,\n",
       "  37,\n",
       "  38,\n",
       "  39,\n",
       "  43,\n",
       "  44,\n",
       "  47,\n",
       "  50,\n",
       "  51,\n",
       "  52,\n",
       "  53,\n",
       "  54,\n",
       "  55,\n",
       "  57,\n",
       "  58,\n",
       "  59,\n",
       "  60,\n",
       "  61,\n",
       "  62,\n",
       "  64,\n",
       "  66,\n",
       "  70,\n",
       "  71,\n",
       "  72,\n",
       "  73,\n",
       "  74,\n",
       "  75,\n",
       "  76,\n",
       "  77,\n",
       "  78,\n",
       "  82,\n",
       "  83,\n",
       "  86,\n",
       "  87,\n",
       "  88,\n",
       "  89,\n",
       "  90,\n",
       "  92,\n",
       "  93,\n",
       "  95,\n",
       "  96,\n",
       "  97,\n",
       "  98]]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def k_means_cluster(rows,distance=pearson,k=4):\n",
    "    # 计算每个维度(特征值)的最小值和最大值，即每一列的最小值和最大值\n",
    "    ranges=[(min([row[i] for row in rows]),max([row[i] for row in rows])) for i in range(len(rows[0]))]\n",
    "    \n",
    "    # 随机创建K个质心\n",
    "    k_center = [[random.random() * ( ranges[i][1] - ranges[i][0]) + ranges[i][0] for i in range(len(rows[0]))] for j in range(k)]\n",
    "    \n",
    "    lastmatches = None\n",
    "    for t in range(100):\n",
    "        # 保存到质心距离最短的最佳文档集\n",
    "        bestmatches = [[] for i in range(k)]\n",
    "        \n",
    "        # 针对每个文档的归属聚类判断开始\n",
    "        # 遍历每一篇文档\n",
    "        for j in range(len(rows)):\n",
    "            # 取得当前文档\n",
    "            row = rows[j]\n",
    "            # 初始化当前文档的最佳聚类ID\n",
    "            bestmatch = 0\n",
    "            # 遍历每一个质心\n",
    "            for i in range(k):\n",
    "                # 计算当前文档与当前质心的距离\n",
    "                dis = distance(row,k_center[i])\n",
    "                # 如果得到的距离小于当前文档与原来最佳质心的距离，则更新最佳的聚类质心\n",
    "                if dis < distance(row, k_center[bestmatch]):\n",
    "                    bestmatch = i\n",
    "            # 将聚类质心ID和文档ID关系出处起来\n",
    "            bestmatches[bestmatch].append(j)\n",
    "        # 针对每个文档的归属聚类判断结束\n",
    "        \n",
    "        # 提前结束\n",
    "        if bestmatches == lastmatches:\n",
    "            break\n",
    "        lastmatches = bestmatches\n",
    "        \n",
    "        # 更新类簇质心开始\n",
    "        # 遍历每一个类簇\n",
    "        for i in range(k):\n",
    "            # 初始化一个列表用来存储更新后的质心坐标\n",
    "            avgs = [0.0] * len(rows[0])\n",
    "\n",
    "            # 如果当前类簇的文档数量大于0\n",
    "            if len(bestmatches[i]) > 0:\n",
    "                # 遍历当前类簇的文档ID集合\n",
    "                for rowid in bestmatches[i]:\n",
    "                    # 计算文档集合中每个维度的总和\n",
    "                    for m in range(len(rows[rowid])):\n",
    "                        avgs[m] += rows[rowid][m]\n",
    "                # 计算每个维度的均值，每个维度的总和/当前类簇的文档数量\n",
    "                for j in range(len(avgs)):\n",
    "                    avgs[j] /= len(bestmatches[i])\n",
    "                # 更新当前类簇为最新的均值\n",
    "                k_center[i] = avgs\n",
    "                    \n",
    "    return bestmatches\n",
    "\n",
    "k_means_cluster(data,k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "tanimoto 系数\n",
    "表示两个向量中的交集与并集的比值\n",
    "v1和v2是相同维度的向量\n",
    "return:\n",
    "返回一个介于0和1之间的数\n",
    "其中1表示每个维度都被两个向量所共有\n",
    "0表示两个向量之间没有交集\n",
    "'''\n",
    "def tanimoto(v1,v2):\n",
    "    c1,c2,shr = 0,0,0\n",
    "    for i in range(len(v1)):\n",
    "        if v1[i] != 0:\n",
    "            c1 += 1\n",
    "        if v2[i] != 0:\n",
    "            c2 += 1\n",
    "        if v1[i] != 0 and v2[i] != 0:\n",
    "            shr += 1\n",
    "    return float(shr)/(c1+c2-shr)\n",
    "# 但是，根据距离函数的意义，即两个向量距离越近，返回的值应该越小，所以改写上述代码\n",
    "def tanimote(v1,v2):\n",
    "    c1,c2,shr = 0,0,0\n",
    "    for i in range(len(v1)):\n",
    "        if v1[i] != 0:\n",
    "            c1 += 1\n",
    "        if v2[i] != 0:\n",
    "            c2 += 1\n",
    "        if v1[i] != 0 and v2[i] != 0:\n",
    "            shr += 1\n",
    "    # 用1减去比率并返回\n",
    "    # 此时，1表示两个向量之间没有交集，距离比较远\n",
    "    # 0表示两个向量交集 == 并集，距离比较近\n",
    "    return 1-float(shr)/(c1+c2-shr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "user,item,data = read_file('zebo.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "result,result1 = hcluster(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      " -\n",
      "  -\n",
      "   -\n",
      "    -\n",
      "     dog\n",
      "     -\n",
      "      cell phone\n",
      "      -\n",
      "       horse\n",
      "       playstation 3\n",
      "    -\n",
      "     -\n",
      "      xbox 360\n",
      "      ps3\n",
      "     -\n",
      "      phone\n",
      "      -\n",
      "       food\n",
      "       computer\n",
      "   -\n",
      "    mansion\n",
      "    -\n",
      "     -\n",
      "      family\n",
      "      friends\n",
      "     -\n",
      "      boyfriend\n",
      "      -\n",
      "       money\n",
      "       -\n",
      "        job\n",
      "        -\n",
      "         -\n",
      "          clothes\n",
      "          shoes\n",
      "         -\n",
      "          dvd player\n",
      "          -\n",
      "           cds\n",
      "           -\n",
      "            psp\n",
      "            tv\n",
      "  -\n",
      "   love\n",
      "   puppy\n",
      " -\n",
      "  house\n",
      "  -\n",
      "   house and lot\n",
      "   -\n",
      "    -\n",
      "     <b>car</b>\n",
      "     -\n",
      "      jeans\n",
      "      -\n",
      "       bike\n",
      "       mobile\n",
      "    -\n",
      "     -\n",
      "      -\n",
      "       mp3 player\n",
      "       digital camera\n",
      "      -\n",
      "       laptop\n",
      "       ipod\n",
      "     -\n",
      "      watch\n",
      "      cellphone\n"
     ]
    }
   ],
   "source": [
    "printclust(result,item)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
